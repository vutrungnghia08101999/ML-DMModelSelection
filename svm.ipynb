{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài toán phân loại sử dụng SVM \n",
    "\n",
    "Mục tiêu: \n",
    "\n",
    "- Xây dựng được mô hình svm sử dụng thư viện sklearn. \n",
    "- Ứng dụng, hiểu cách áp dụng mô hình svm vào giải quyết bài toán thực tế (vd: phân loại văn bản) \n",
    "- Sử dụng độ đo Accuracy để làm độ đo đánh giá chất lượng mô hình. \n",
    "\n",
    "Vấn đề: \n",
    "- Có một tập các văn bản dạng text không có nhãn, làm sao để biết văn bản này là thuộc về thể loại nào, pháp luật, đời sống, văn học, thể thao ...\n",
    "- => Xây dựng mô hình học máy có thể phân loại các thể loại của văn bản chỉ dựa trên nội dung.  \n",
    "\n",
    "Dữ liệu: \n",
    "- Có tập các văn bản và nhãn tương ứng của từng văn bản trong một khoảng thời gian \n",
    "- Tập các nhãn - 10 nhãn văn bản: \n",
    "    > Giải trí, Khoa học - Công nghệ, Kinh tế, Pháp luật, Sức khỏe, Thể thao, Thời sự, Tin khác, Độc giả, Đời sống - Xã hội\n",
    "- Ví dụ văn bản nhãn **thể thao**: \n",
    "    > \"Dân_trí Real Madrid đã dẫn trước trong cả trận đấu , nhưng họ vẫn phải chấp_nhận bị Dortmund cầm hòa 2-2 ở Bernabeu . Real Madrid chấp_nhận đứng thứ_hai ở bảng F Champions League ...\"\n",
    "\n",
    "Bài toán: \n",
    "- Input: tập các từ trong văn bản 1 mẫu dữ liệu $X = [x_1, x_2, ... x_n]$\n",
    "- Output: nhãn $y$ là 1 trong 10 nhãn trên "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vutrungnghia/miniconda3/envs/svm/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.datasets.base import load_files\n",
    "from pyvi import ViTokenizer\n",
    "from sklearn import svm\n",
    "from sklearn.datasets.base import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dữ liệu từ thư mục đã crappy từ trước \n",
    "\n",
    "Cấu trúc thư mục như sau \n",
    "\n",
    "- data/news_1135/\n",
    "\n",
    "    - Kinh tế: \n",
    "        - bài báo 1.txt \n",
    "        - bài báo 2.txt \n",
    "    - Pháp luật\n",
    "        - bài báo 3.txt \n",
    "        - bài báo 4.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng văn bản    Nhãn                          \n",
      "---------------------------------------------\n",
      "107                 Giải trí                    \n",
      "196                 Khoa học - Công nghệ      \n",
      "186                 Kinh tế                     \n",
      "50                  Pháp luật                  \n",
      "75                  Sức khỏe                   \n",
      "140                 Thể thao                    \n",
      "138                 Thời sự                   \n",
      "100                 Tin khác                     \n",
      "91                  Đời sống - Xã hội      \n",
      "52                  Độc giả                    \n",
      "---------------------------------------------\n",
      "Tổng số văn bản: 1135\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"data/news_1135/\"\n",
    "header = \"%-20s%-30s\" % (\"Số lượng văn bản\", \"Nhãn\")\n",
    "print(header)\n",
    "print(\"---------------------------------------------\")\n",
    "total = 0\n",
    "for label in os.listdir(DATA_PATH):\n",
    "    n = len(os.listdir(os.path.join(DATA_PATH, label)))\n",
    "    total += n\n",
    "    entry = \"%-20d%-30s\" % (n, label)\n",
    "    print(entry)\n",
    "print(\"---------------------------------------------\")\n",
    "print(f'Tổng số văn bản: {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n"
     ]
    }
   ],
   "source": [
    "data_train = load_files(container_path=DATA_PATH, encoding=\"utf-8\")\n",
    "print(dir(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID     Nhãn      \n",
      "---------------------------------------------\n",
      "0      Giải trí\n",
      "1      Khoa học - Công nghệ\n",
      "2      Kinh tế \n",
      "3      Pháp luật\n",
      "4      Sức khỏe\n",
      "5      Thể thao\n",
      "6      Thời sự\n",
      "7      Tin khác \n",
      "8      Đời sống - Xã hội\n",
      "9      Độc giả\n"
     ]
    }
   ],
   "source": [
    "header = \"%-6s %-10s\" % (\"ID\", \"Nhãn\")\n",
    "print(header)\n",
    "print(\"---------------------------------------------\")\n",
    "for id, label in enumerate(data_train.target_names):\n",
    "    print(\"%-6d %-10s\" % (id, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dân_trí Sở GD & ĐT tỉnh Gia_Lai vừa ra văn_bản số 2258 / SGDĐT - VP , về việc chấn_chỉnh việc tiếp_thị sách và các vật_dụng khác trong các cơ_sở giáo_dục . Văn_bản chỉ_đạo , tuyệt_đối không cho phép các cá_nhân , tập_thể đến trường tiếp_thị , quảng_cáo mua_bán sách , dụng_cụ học_tập … cho giáo_viên và học_sinh trong nhà_trường . Các tổ_chức , cá_nhân trong ngành giáo_dục tuyệt_đối không được thực_hiện hoặc tham_gia giới_thiệu , quảng_bá , vận_động mua , phát_hành sách tham_khảo tới học_sinh hoặc phụ_huynh dưới hình_thức nào . Nhà_trường tuyệt_đối không được lưu_hành , sử_dụng sách có nội_dung không lành_mạnh , không phù_hợp với nội_dung chương_trình phổ_thông . Trường_hợp phát_hiện sách có sai_sót , các đơn_vị cần báo_cáo với cấp trên để có hướng xử_lý . Các sơ sở giáo_dục đề_cao cảnh_giác đối_với trường_hợp mạo_danh cán_bộ , chuyên_viên sở trong ngành đi giới_thiệu sách , đồ_dùng học_sinh ; công_khai phổ_biến các quy_định trên đến cán_bộ , giáo_viên , học_sinh để cùng phòng tránh và ngăn_chặn … Trước đó , báo Dân_trí đã thông_tin về việc học_sinh của Trường Tiểu_học số 2 xã Hòa Phú ( Chư_Păh , Gia_Lai ) đã mang 1 tờ giấy thông_báo về việc mua sách tham_khảo mang về cho phụ_huynh và xin tiền để mua sách , khiến nhiều phụ_huynh bức_xúc . Sự_việc được bà Dương Thị Nga - Hiệu_trưởng nhà_trường cho biết , do hôm xảy ra sự_việc , bà đi_vắng nên không hay_biết . Tuệ Mẫn', 'Dân_trí Làm thế_nào để chim non học hót theo một_cách vừa bảo_đảm cả tính độc_đáo của văn hóa địa_phương và vừa đảm_bảo đặc_điểm theo loài ? Các nhà nghiên_cứu đã bắt_đầu lập bản_đồ các mạch nơron thần_kinh chịu trách_nhiệm truyền_tải văn hóa và đặc_điểm loài trong tiếng chim hót . Hai nghiên_cứu công_bố trên ấn_bản ngày 09 tháng 12 của tạp_chí Khoa_học đã làm sáng_tỏ các cấu \\u200b trúc thần_kinh của phần não_bộ học hót của chim . Trong một thí_nghiệm , Tiến_sĩ Vikram Gadagkar , nhà nghiên_cứu sau tiến_sĩ và là nhà sinh_học thần_kinh tại Đại_học Cornell , và các đồng_nghiệp đã phát_hiện ra rằng tế_bào thần_kinh dopaminergic ở vùng não VTA ( Ventral Tegmental Area ) đã mã_hóa các khác lệch trong hoạt_động hót . Các tín_hiệu dopaminergic này cũng có_thể giúp chim_sẻ vằn vị_thành_niên_học cách bắt_chước các tiếng hót của các con chim trưởng_thành . Trong nghiên_cứu thứ hai , các nhà điều_tra nghiên_cứu trường_hợp con chim được nuôi_dưỡng bởi các loài khác . Tiến_sĩ Makoto Araki , Viện Khoa_học và Công_nghệ Đại_học nghiên_cứu Okinawa , Nhật_Bản và các đồng_nghiệp xác_định rằng , trong khi chim_sẻ vằn vị_thành_niên bắt_chước các âm_tiết “ bài_hát ” của cha_mẹ họ sẻ Bengalese nuôi của chúng , chúng vẫn điều_chỉnh nhịp_điệu bài_hát theo đặc_trưng loài riêng của chúng , với “ ca_khúc ” mà chúng chưa bao_giờ nghe , cho thấy rằng loài chim biết hót học nhịp_điệu từ một mẫu bẩm_sinh chứ không phải là từ các con chim khác . Trong cùng ấn_bản trên tạp_chí Khoa_học , tiến_sĩ Ofer Tchernichovski , Dina Lipkind , và các nhà nghiên_cứu tâm_lý học tại Đại_học Hunter , Đại_học Thành_phố New_York ( CUNY ) , đưa_ra quan_điểm về các nghiên_cứu trên . Tiến_sĩ . Tchernichovski và Lipkind , những người đã không hợp_tác với một trong hai nghiên_cứu , đề_xuất rằng các kết_quả có_thể làm sáng_tỏ về cách_thức mà các loài chim duy_trì dấu_hiệu đặc_trưng loài về giọng họt bất_chấp những thay_đổi ngẫu_nhiên xảy ra trong quần thể địa_phương và được tích_lũy qua nhiều thế_hệ . Theo các nhà nghiên_cứu đại_học Hunter , hai loại tế_bào thần_kinh ở vỏ_não thính_giác của loài chim biết hót có_thể mã độc_lập cho âm_thanh của bài_hát và nhịp_điệu – trong đó bài_hát có xu_hướng phụ_thuộc đầu_vào từ những chim trưởng_thành hướng_dẫn và nhịp nhịp_điệu từ một mẫu bẩm_sinh hoặc \" mã_vạch \" sẵn_có . Trong_khi các nhà khoa_học chỉ mới bắt_đầu hiểu được cơ_chế thần_kinh hỗ_trợ việc học thanh_nhạc ở chim biết hót , tiến_sĩ . Tchernichovski và Lipkind chỉ ra rằng nghiên_cứu này là có liên_quan đến nhiều hệ_thống truyền_thông động_vật , bao_gồm cả chuyển_tiếp văn hóa ổn_định ở loài người . Tiến_sĩ Tchernichovski đứng_đầu phòng_thí_nghiệm của học_tập về giọng_nói tại Đại_học Hunter , Đại_học Thành_phố NewYork , đã sử_dụng các loài chim biết hót để nghiên_cứu các cơ_chế của việc học thanh_nhạc . Giống_như sự phát_triển lời_nói đầu ở trẻ sơ_sinh , các loài chim biết hót học cách bắt_chước những âm_thanh phức_tạp trong một giai_đoạn quan_trọng của sự phát_triển . Những con chim lớn không_thể bắt_chước được nữa – và chúng_ta chưa biết tại_sao . Phòng_thí_nghiệm của_ông nghiên_cứu về hành_vi động_vật và động_lực của việc học thanh_nhạc và việc tạo ra âm_thanh qua các cấp_độ não_bộ khác nhau . Các thí_nghiệm nhằm tìm_ra các quy_trình não sinh_lý và phân_tử ( biệu hiện gen ) là cơ_sở cho việc học_tập các bài_hát . Nhã Khanh ( Theo Sciencedaily )']\n",
      "\n",
      "['data/news_1135/Tin khác/0218e1df21ce358b9c6485176a48f1fcaeedef67.txt'\n",
      " 'data/news_1135/Khoa học - Công nghệ/bf9889f5f2ffd6c92fa877d35ef0ef5f34f0666d.txt']\n",
      "\n",
      "[7 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_train.data[0:2], end='\\n\\n')\n",
    "print(data_train.filenames[0:2], end='\\n\\n')\n",
    "print(data_train.target[0:2], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135\n",
      "1135\n",
      "1135\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Bài tập\n",
    " - Kiểm tra các thông tin sau:\n",
    "    + Số lượng văn bản trong data_train.data\n",
    "    + Số lượng ids trong data_train.target\n",
    "    + Số lượng filenames trong data_train.filenames\n",
    "\"\"\"\n",
    "###############\n",
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tiền xử lý dữ liệu đưa dữ liệu từ dạng text về dạng ma trận bằng TF-IDF\n",
    "\n",
    "- Thử nghiệm để kiểm tra hoạt động chuyển hoá dữ liệu về dạng ma trận "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số lượng từ dừng: 2063\n",
      "Danh sách 10 từ dừng đầu tiên (từ không mang ý nghĩa phân loại):  ['a_lô', 'a_ha', 'ai', 'ai_ai', 'ai_nấy', 'ai_đó', 'alô', 'amen', 'anh', 'anh_ấy']\n",
      "\n",
      "5 từ đầu tiên trong từ điển:\n",
      "\n",
      "1 :  ('dân_trí', 6928)\n",
      "2 :  ('sở', 17869)\n",
      "3 :  ('gd', 7729)\n",
      "4 :  ('đt', 23214)\n",
      "5 :  ('tỉnh', 20851)\n",
      "6 :  ('gia_lai', 7816)\n",
      "\n",
      "Số chiều của dữ liệu: (1135, 24389)\n",
      "Số từ trong từ điển: 24389\n"
     ]
    }
   ],
   "source": [
    "# load dữ liệu các stopwords \n",
    "with open(\"data/vietnamese-stopwords.txt\") as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [x.strip().replace(\" \", \"_\") for x in stopwords]\n",
    "print(f\"Tổng số lượng từ dừng: {len(stopwords)}\")\n",
    "print(\"Danh sách 10 từ dừng đầu tiên (từ không mang ý nghĩa phân loại): \", stopwords[:10])\n",
    "print()\n",
    "\n",
    "\"\"\"\n",
    "Chuyển hoá dữ liệu text về dạng vector tfidf \n",
    "    - loại bỏ từ dừng\n",
    "    - sinh từ điển\n",
    "    - chuyển thành dữ liệu dạng ma trận 2 chiều kích thước n x m với n là số lượng văn bản và m là số lượng từ trong từ điển\n",
    "\"\"\"\n",
    "module_count_vector = CountVectorizer(stop_words=stopwords)\n",
    "model_rf_preprocess = Pipeline([('vect', module_count_vector),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ])\n",
    "data_preprocessed = model_rf_preprocess.fit_transform(data_train.data, data_train.target)\n",
    "print(\"5 từ đầu tiên trong từ điển:\\n\")\n",
    "i = 0\n",
    "for k,v in module_count_vector.vocabulary_.items():\n",
    "    i+=1\n",
    "    print(i, \": \", (k, v))\n",
    "    if i > 5:\n",
    "        break \n",
    "print()\n",
    "\n",
    "# Số chiều của dữ liệu \n",
    "print(f\"Số chiều của dữ liệu: {data_preprocessed.shape}\")\n",
    "print(f\"Số từ trong từ điển: {len(module_count_vector.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chia dữ liệu thành 2 phần train_data và test_data\n",
    "- train_data chiếm 80 % dữ liệu \n",
    "- test_data chiếm 20 % dữ liệu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu training =  (908, 24389) (908,)\n",
      "Dữ liệu testing =  (227, 24389) (227,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# chia dữ liệu thành 2 phần sử dụng hàm train_test_split.\n",
    "test_size = 0.2\n",
    "# cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split( data_preprocessed, data_train.target, test_size=test_size)\n",
    "\n",
    "\n",
    "# hiển thị một số thông tin về dữ liệu \n",
    "print(\"Dữ liệu training = \", X_train.shape, y_train.shape)\n",
    "print(\"Dữ liệu testing = \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Sức khỏe\n",
      "6 Thời sự\n",
      "5 Thể thao\n",
      "6 Thời sự\n",
      "4 Sức khỏe\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Bài tập\n",
    " - Hiển thị ra id, tên nhãn của 5 văn bản đầu tiên trong tập train. \n",
    " - Gợi ý: lấy dữ liệu id từ biến y_train, mapping với thứ tự nằm trong mảng data_train.target_names\n",
    "\"\"\"\n",
    "###############\n",
    "###############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Huấn luyện mô hình SVM trên tập train_data\n",
    "\n",
    "Sử dụng thư viện sklearn để xây dựng mô hình \n",
    "- `svm.SVC(kernel='linear', C=1.0)`: chọn hàm nhân phân tách là linear, tham số C=1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Training ...\n",
      "- Train size = (908, 24389)\n",
      "- model - train complete\n"
     ]
    }
   ],
   "source": [
    "print(\"- Training ...\")\n",
    "\n",
    "\n",
    "# X_train.shape\n",
    "print(\"- Train size = {}\".format(X_train.shape))\n",
    "model = svm.SVC(kernel='linear', C=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"- model - train complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Đánh giá mô hình SVM trên tập test_data\n",
    "\n",
    "Thực hiện dự đoán nhãn cho từng văn bản trong tập test \n",
    "\n",
    "Độ đo đánh giá: \n",
    "> accuracy = tổng số văn bản dự đoán đúng  / tổng số văn bản có trong tập test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Testing ...\n",
      "- Acc = 0.8414096916299559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"- Testing ...\")\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"- Acc = {}\".format(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Thực hiện sử dụng model đã được train để suy diễn 1 văn bản mới \n",
    "- Dữ liệu mới đến ở dạng dữ liệu thô => cần tiền xử lý dữ liệu về dạng dữ_liệu_ma_trận\n",
    "- infer sử dụng hàm model.predict(dữ_liệu_ma_trận) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 24149)\t0.6411817745645509\n",
      "  (0, 21498)\t0.327365802861761\n",
      "  (0, 7776)\t0.4797308142038527\n",
      "  (0, 3408)\t0.5015734332939062\n",
      "\n",
      "[5] Thể thao\n"
     ]
    }
   ],
   "source": [
    "# tiền xử lý dữ liệu sử dụng module model_rf_preprocess. \n",
    "news = [\"Công_phượng ghi bàn cho đội_tuyển Việt_nam\"]\n",
    "preprocessed_news = model_rf_preprocess.transform(news)\n",
    "print(preprocessed_news, end='\\n\\n')\n",
    "pred = model.predict(preprocessed_news)\n",
    "print(pred, data_train.target_names[pred[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bài tập bổ sung: \n",
    "\n",
    "### 4.1 Thử nghiệm các tham số \n",
    "\n",
    "- Các tham số với giá trị khác nhau có thể ảnh hưởng để kết quả học \n",
    "- Cần thử nghiệm kỹ lượng để đưa ra kết quả khách quan: tham số C, kernel.\n",
    "    - Chọn mô hình với bộ tham số cho kết quả tốt nhất "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bài tập\n",
    " - Đánh giá các tham số của mô hình SVM: kernel, C\n",
    " - Gợi ý:\n",
    "     + Đầu tiên cố định C = 1.0 (có thể là giá trị khác), thay đổi kernel = {'linear', 'poly', 'rbf', 'sigmoid'}\n",
    "     + Với mỗi kernel chạy huấn luyện và đánh giá lại mô hình. Chọn kernel cho acc cao nhất.\n",
    "       Giả sử trong trường hợp này là linear\n",
    "     + Cố định kernel là linear, thay đổi C = {0.1, 1.0, 5.0, 10.0}\n",
    "     + Với mỗi giá trị C chạy huấn luện và đánh giá lại. Chọn C cho acc cao nhất.\n",
    "\"\"\"\n",
    "######################\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Phân loại số viết tay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu training =  (1437, 64) (1437,)\n",
      "Dữ liệu testing =  (360, 64) (360,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "target = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split( data, target, test_size=test_size)\n",
    "\n",
    "print(\"Dữ liệu training = \", X_train.shape, y_train.shape)\n",
    "print(\"Dữ liệu testing = \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bài tập\n",
    " - Đánh giá các tham số của mô hình SVM với bài toán phân loại ảnh\n",
    " - Gợi ý: Làm tương tự với phân loại văn bản phía trên\n",
    "\"\"\"\n",
    "######################\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
